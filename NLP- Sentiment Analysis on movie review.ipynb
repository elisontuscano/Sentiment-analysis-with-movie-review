{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "# LIbrary to Clean the texts\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First of all, Jenna Jameson is the best actres...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once in a while in Indian cinema there comes a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I've just finished listening to the director's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This was surprisingly intelligent for a TV mov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This is one of the worst movies I saw! I dunno...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  First of all, Jenna Jameson is the best actres...  0\n",
       "1  Once in a while in Indian cinema there comes a...  0\n",
       "2  I've just finished listening to the director's...  1\n",
       "3  This was surprisingly intelligent for a TV mov...  1\n",
       "4  This is one of the worst movies I saw! I dunno...  0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=[]\n",
    "for filename in glob.glob(os.path.join('train/pos/', '*.txt')):\n",
    "    f = open(filename, 'r')\n",
    "    content = f.read()\n",
    "    df.append([content,1]) #1 stands for positive review\n",
    "    \n",
    "for filename in glob.glob(os.path.join('train/neg/', '*.txt')):\n",
    "    f = open(filename, 'r')\n",
    "    content = f.read()\n",
    "    df.append([content,0]) #0 stands for negative review\n",
    "    \n",
    "train=pd.DataFrame(df)\n",
    "train=train.sample(frac=1).reset_index(drop=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*** Spoiler in fifth paragraph *** This was an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a horrific re-make of the French movie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of the Message Boards threads at IMDb had ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Life is too short to waste on two hours of Hol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This interesting lost film (written by Terence...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  *** Spoiler in fifth paragraph *** This was an...  1\n",
       "1  This is a horrific re-make of the French movie...  0\n",
       "2  One of the Message Boards threads at IMDb had ...  1\n",
       "3  Life is too short to waste on two hours of Hol...  0\n",
       "4  This interesting lost film (written by Terence...  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=[]\n",
    "for filename in glob.glob(os.path.join('test/pos/', '*.txt')):\n",
    "    f = open(filename, 'r')\n",
    "    content = f.read()\n",
    "    df.append([content,1]) #1 stands for positive review\n",
    "    \n",
    "for filename in glob.glob(os.path.join('test/neg/', '*.txt')):\n",
    "    f = open(filename, 'r')\n",
    "    content = f.read()\n",
    "    df.append([content,0]) #0 stands for negative review\n",
    "    \n",
    "test=pd.DataFrame(df)\n",
    "test=test.sample(frac=1).reset_index(drop=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def getCleanReview(review):\n",
    "    review = remove_html_tags(review)\n",
    "    #only keep alphabets remove rest\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    #turn all reviews into lowercase\n",
    "    review = review.lower()\n",
    "    #remove stopwords and do stemming\n",
    "    #review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    #Lemmatize and merge the review together after making all the changes\n",
    "    #review = ' '.join([lemmatizer.lemmatize(word) for word in review])\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train.iloc[:,0].values\n",
    "Y_train=train.iloc[:,1].values\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i]=getCleanReview(X_train[i])\n",
    "    \n",
    "#clean X_test as well \n",
    "X_test=test.iloc[:,0].values\n",
    "Y_test=test.iloc[:,1].values\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i]=getCleanReview(X_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74218"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordfreq={}\n",
    "for review in X_train:\n",
    "    words=review.split()\n",
    "    for word in words:\n",
    "        if word not in wordfreq.keys():\n",
    "            wordfreq[word]=1\n",
    "        else:\n",
    "            wordfreq[word]+=1\n",
    "len(wordfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28757"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove words with occurance less than 5 and remove stopwords\n",
    "repeat=[]\n",
    "for i in wordfreq.keys():\n",
    "    if wordfreq[i] <5 :\n",
    "        repeat.append(i)\n",
    "    \n",
    "for i in repeat:\n",
    "    del wordfreq[i]\n",
    "\n",
    "len(wordfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the less frequent words from out training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_words(dataset,wordfreq):\n",
    "    for index ,review in enumerate(dataset):\n",
    "        cleanreview=[]\n",
    "        for word in review.split():\n",
    "            if word in wordfreq.keys():\n",
    "                cleanreview.append(word)\n",
    "        cleanreview=' '.join(cleanreview)\n",
    "        dataset[index]=cleanreview\n",
    "    return dataset\n",
    "\n",
    "X_train=remove_extra_words(X_train,wordfreq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions required for naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of documents containing the word\n",
    "def Word_count(X,wordfreq):\n",
    "    wordcount={}\n",
    "    for word in wordfreq.keys():\n",
    "        count=0\n",
    "        for review in X:\n",
    "            if word in review:\n",
    "                count+=1\n",
    "        wordcount[word]=count\n",
    "    return wordcount\n",
    "\n",
    "wordcount=Word_count(X_train,wordfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#occurance of the word in positive document\n",
    "def Word_positive(X,Y,wordfreq):\n",
    "    wordpositive={}\n",
    "    for word in wordfreq.keys():\n",
    "        count=0\n",
    "        for index,review in enumerate(X):\n",
    "            if word in review and Y[index]==1:\n",
    "                count+=1\n",
    "        wordpositive[word]=count\n",
    "    return wordpositive\n",
    "\n",
    "wordpositive=Word_positive(X_train,Y_train,wordfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#occurance of the word in negative document\n",
    "def Word_negative(X,Y,wordfreq):\n",
    "    wordnegative={}\n",
    "    for word in wordfreq.keys():\n",
    "        count=0\n",
    "        for index,review in enumerate(X):\n",
    "            if word in review and Y[index]==0:\n",
    "                count+=1\n",
    "        wordnegative[word]=count\n",
    "    return wordnegative\n",
    "\n",
    "wordnegative=Word_negative(X_train,Y_train,wordfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_sentiment(Y ,sentiment):\n",
    "    length=0\n",
    "    for i in Y:\n",
    "        if i == sentiment:\n",
    "            length+=1\n",
    "    return length\n",
    "\n",
    "def conditional_probability(X,Y,word,sentiment,length):\n",
    "    if sentiment ==1:\n",
    "        count=0 if word not in wordpositive.keys() else wordpositive[word]\n",
    "    else:\n",
    "        count=0 if word not in wordnegative.keys() else wordnegative[word]\n",
    "    return count/length\n",
    "\n",
    "def get_sentiment_probability(Y,sentiment):\n",
    "    count=0\n",
    "    for i in Y:\n",
    "        if i==sentiment:\n",
    "            count+=1\n",
    "    return count/len(Y)\n",
    "\n",
    "def calculateBayes(X,Y,word,sentiment,length):\n",
    "    one=conditional_probability(X,Y,word,sentiment,length)\n",
    "    two=length/len(Y) #probability of the sentiment\n",
    "    three=calculate_probability_occurance(X,word)\n",
    "    count=wordcount[word] if word in wordcount.keys() else 2\n",
    "    return  (((one*two)+1)/(three+count))\n",
    "\n",
    "def reviewSentiment(X,Y,review,sentiment):\n",
    "    sum=1\n",
    "    length = total_sentiment(Y_train ,1)\n",
    "    for word in review.split():\n",
    "        prob = calculateBayes(X,Y,word,sentiment,length)\n",
    "        sum *= prob\n",
    "    return sum\n",
    "\n",
    "def naiveBayes(X,Y,review):\n",
    "    #check if review sentiment is positive\n",
    "    positive =reviewSentiment(X,Y,review,1)\n",
    "    #check if review sentiment is negative\n",
    "    negative =reviewSentiment(X,Y,review,0)\n",
    "    if positive > negative:\n",
    "        return 1\n",
    "    else :\n",
    "        return 0\n",
    "    \n",
    "def naiveBayes_onWhole(X,Y,x_test):\n",
    "    ypred=[]\n",
    "    for review in x_test:\n",
    "        pred=naiveBayes(X,Y,review)\n",
    "        ypred.append(pred)\n",
    "    return ypred\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual)) * 100.0\n",
    "\n",
    "def calculate_probability_occurance(dataset,word):\n",
    "    count= 0 if word not in wordcount.keys() else wordcount[word]\n",
    "    return count/len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate accuracy on test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy By Naive Bayes from scratch on Test set : 52.144 %\n"
     ]
    }
   ],
   "source": [
    "X_test=remove_extra_words(X_test,wordfreq)\n",
    "y_pred=naiveBayes_onWhole(X_train,Y_train,X_test)\n",
    "score=accuracy_metric(Y_test, y_pred)\n",
    "print('Accuracy By Naive Bayes from scratch on Test set : {} %'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Naive Bayes library from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train.iloc[:,0].values\n",
    "Y_train=train.iloc[:,1].values\n",
    "for i in range(len(X_train)):\n",
    "    X_train[i]=getCleanReview(X_train[i])\n",
    "X_test=test.iloc[:,0].values\n",
    "Y_test=test.iloc[:,1].values\n",
    "for i in range(len(X_test)):\n",
    "    X_test[i]=getCleanReview(X_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 10000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up tfidfvectorizor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizor=TfidfVectorizer(stop_words='english', max_df=0.7,max_features=10000)\n",
    "\n",
    "#fit and transform train and test set\n",
    "tfidf_train=tfidf_vectorizor.fit_transform(X_train).toarray()\n",
    "tfidf_test=tfidf_vectorizor.transform(X_test).toarray()\n",
    "tfidf_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy By Naive Bayes using library on Test set : 78.46400000000001 %\n"
     ]
    }
   ],
   "source": [
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "NaiveClassifier = GaussianNB()\n",
    "NaiveClassifier.fit(tfidf_train, Y_train)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = NaiveClassifier, X = tfidf_train, y = Y_train, cv = 10)\n",
    "print('Accuracy By Naive Bayes using library on Test set : {} %'.format(accuracies.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elisontuscano/opt/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['model/tfidf_model.sav']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "#saving the model\n",
    "joblib.dump(NaiveClassifier,'model/NaiveBayes_model.sav')\n",
    "joblib.dump(tfidf_vectorizor,'model/tfidf_model.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Random Forest Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy By Random Forest on Test set : 79.208 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "RFClassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "RFClassifier.fit(tfidf_train, Y_train)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "accuracies = cross_val_score(estimator = RFClassifier, X = tfidf_train, y = Y_train, cv = 10)\n",
    "print('Accuracy By Random Forest on Test set : {} %'.format(accuracies.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7867999999999999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the accuracy on test data\n",
    "accuracies = cross_val_score(estimator =RFClassifier, X = tfidf_test, y = Y_test, cv = 10)\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement with Artificial neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/elisontuscano/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=10000, units=256, kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "/Users/elisontuscano/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=64, kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/elisontuscano/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  del sys.path[0]\n",
      "/Users/elisontuscano/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.3178 - accuracy: 0.8635 - val_loss: 0.3194 - val_accuracy: 0.8625\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.1618 - accuracy: 0.9365 - val_loss: 0.3725 - val_accuracy: 0.8584\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 0.0623 - accuracy: 0.9764 - val_loss: 0.6043 - val_accuracy: 0.8516\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0129 - accuracy: 0.9940 - val_loss: 1.1866 - val_accuracy: 0.8438\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.0039 - accuracy: 0.9984 - val_loss: 1.4588 - val_accuracy: 0.8461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b2986d650>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim = 256, init = 'uniform', activation = 'relu', input_dim = 10000))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(output_dim = 64, init = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(tfidf_train, Y_train, batch_size = 10, nb_epoch = 5,validation_data=(tfidf_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy Using ANN: 84.61199998855591 %\n"
     ]
    }
   ],
   "source": [
    "classifier.save('model/ann_model.h5')\n",
    "score = classifier.evaluate(tfidf_test, Y_test, verbose=0)\n",
    "print('Test accuracy Using ANN: {} %'. format(score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
